{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data for Modeling\n",
    "\n",
    "**Wrangle**\n",
    "\n",
    "1. Acquired data from student-mat.csv. \n",
    "\n",
    "2. Create dummy vars\n",
    "\n",
    "3. Split data \n",
    "\n",
    "4. Scale data\n",
    "\n",
    "wrangle.wrangle_student_math(path) returns the following: \n",
    "\n",
    "| Object Returned | Description | Purpose                          |\n",
    "|:-------------------|:--------------------------------|:----------------------------------------------------|\n",
    "| 1. df | **Dataframe**, **Feature** and **target** variables, **Unscaled**, Dummy vars **with** original categorical vars | New features, additional cleaning needed, etc. |\n",
    "| 2. X_train_exp | **Dataframe**, **Feature** variables only, **Unscaled**, Dummy vars **with** original categorical vars | Exploration & analysis     |\n",
    "| 3. X_train | **Dataframe**, **Feature** variables only, **Scaled**, Dummy vars **without** original categorical vars | Feature selection, fit models, make predictions |\n",
    "| 4. y_train | **Series**, **Target** variable only, **Unscaled** | Feature selection, evaluate model predictions |\n",
    "| 5. X_validate | **Dataframe**, **Features** variables only, **Scaled**, Dummy vars **without** original categorical vars | Make predictions using top models |\n",
    "| 6. y_validate | **Series**, **Target** variable only, **Unscaled** | Evaluate model predictions made from X_validate to assess overfitting | \n",
    "| 7. X_test | **Dataframe**, **Features** variables only, **Scaled**, Dummy vars **without** original categorical vars | Make predictions using best model|\n",
    "| 8. y_test | **Series**, **Target** variable only, **Unscaled** | Evaluate model predictions made from X_test to estimate future performance on new data |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import wrangle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# modeling methods\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import LinearRegression, LassoLars, TweedieRegressor\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "path='https://gist.githubusercontent.com/ryanorsinger/55ccfd2f7820af169baea5aad3a9c60d/raw/da6c5a33307ed7ee207bd119d3361062a1d1c07e/student-mat.csv'\n",
    "\n",
    "df, \\\n",
    "X_train_exp, \\\n",
    "X_train, \\\n",
    "y_train, \\\n",
    "X_validate, \\\n",
    "y_validate, \\\n",
    "X_test, \\\n",
    "y_test = wrangle.wrangle_student_math(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the head of our X:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Target Variable/y**\n",
    "\n",
    "This helps us determine which type of algorithm we may want to use. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how do we determine distribution?\n",
    "plt.hist(y_train)\n",
    "plt.title('Distribution of Target (G3)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cursory conclusion: distribution of the target G3 looks roughly normal, will test against generalized linear models to \n",
    "# see if we find a better fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling\n",
    "\n",
    "### Baseline\n",
    "\n",
    "About the initial baseline: \n",
    "\n",
    "> Before we begin making models, we need to know how well we can estimate (predict) the final grade (G3) without using any features. This is often done by predicting every observation's target value to be the mean or the median. E.g. we could predict every student's final grade to be the mean final grade of all the students in our training sample. We will try both the mean and the median, see which performs best, and set that evaluation metric value as our baseline performance to beat. \n",
    "\n",
    "\n",
    "1. Predict all final grades to be 10.52, which is equal to the mean of G3 for the training sample. Store in `y_train['G3_pred_mean']`. \n",
    "\n",
    "2. Predict all final grades to be 11, which is equal to the median of G3 for the training sample. Store in `y_train['G3_pred_median']`.  \n",
    "\n",
    "3. Compute the RMSE comparing actual final grade (G3) to G3_pred_mean. \n",
    "\n",
    "4. Compute the RMSE comparing actual final grade (G3) to G3_pred_median. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can change y_train and y_validate to be dataframes to append the new columns with predicted values. \n",
    "\n",
    "\n",
    "# 1. Predict G3_pred_mean\n",
    "\n",
    "\n",
    "# 2. compute G3_pred_median\n",
    "\n",
    "\n",
    "# 3. RMSE of G3_pred_mean\n",
    "\n",
    "print(\"RMSE using Mean\\nTrain/In-Sample: \", round(rmse_train, 2), \n",
    "      \"\\nValidate/Out-of-Sample: \", round(rmse_validate, 2))\n",
    "\n",
    "# 4. RMSE of G3_pred_median\n",
    "# rmse_train = mean_squared_error(y_train.G3, y_train.G3_pred_median) ** .5\n",
    "# rmse_validate = mean_squared_error(y_validate.G3, y_validate.G3_pred_median) ** .5\n",
    "# print(\"RMSE using Median\\nTrain/In-Sample: \", round(rmse_train, 2), \n",
    "#       \"\\nValidate/Out-of-Sample: \", round(rmse_validate, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import explained_variance_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot to visualize actual vs predicted. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LinearRegression (OLS)\n",
    "\n",
    "1. Fit the model using X_train_scaled and the labels from y_train. \n",
    "\n",
    "2. Predict final grade for students in training sample using our model (lm). \n",
    "\n",
    "3. Evaluate using RMSE\n",
    "\n",
    "4. Repeat predictions and evaluation for validation. \n",
    "\n",
    "5. Compare RMSE train vs. validation. Overfitting? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the model object\n",
    "# \n",
    "# make the thing\n",
    "# \n",
    "\n",
    "\n",
    "# fit the model to our training data. We must specify the column in y_train, \n",
    "# since we have converted it to a dataframe from a series! \n",
    "# \n",
    "# fit the thing\n",
    "# \n",
    "\n",
    "# predict train\n",
    "# \n",
    "# use the thing!\n",
    "# \n",
    "\n",
    "# evaluate: rmse\n",
    "# predict validate\n",
    "\n",
    "# evaluate: rmse\n",
    "\n",
    "print(\"RMSE for OLS using LinearRegression\\nTraining/In-Sample: \", rmse_train, \n",
    "      \"\\nValidation/Out-of-Sample: \", rmse_validate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######## \n",
    "# ~~~~~~~~~addendum~~~~~~~\n",
    "#  we will incrementally build \n",
    "# a dataframe for comparison of \n",
    "# our metrics for model selection\n",
    "######\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LassoLars\n",
    "\n",
    "1. Fit the model using X_train_scaled and the labels from y_train. \n",
    "\n",
    "2. Predict final grade for students in training sample using our model (lars). \n",
    "\n",
    "3. Evaluate using RMSE\n",
    "\n",
    "4. Repeat predictions and evaluation for validation. \n",
    "\n",
    "5. Compare RMSE train vs. validation. Overfitting? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create the model object\n",
    "\n",
    "# fit the model to our training data. We must specify the column in y_train, \n",
    "# since we have converted it to a dataframe from a series!\n",
    "\n",
    "# predict train\n",
    "\n",
    "# evaluate: rmse\n",
    "\n",
    "# predict validate\n",
    "\n",
    "# evaluate: rmse\n",
    "\n",
    "print(\"RMSE for Lasso + Lars\\nTraining/In-Sample: \", rmse_train, \n",
    "      \"\\nValidation/Out-of-Sample: \", rmse_validate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TweedieRegressor (GLM)\n",
    "\n",
    "1. Fit the model using X_train_scaled and the labels from y_train. \n",
    "\n",
    "2. Predict final grade for students in training sample using our model (glm). \n",
    "\n",
    "3. Evaluate using RMSE\n",
    "\n",
    "4. Repeat predictions and evaluation for validation. \n",
    "\n",
    "5. Compare RMSE train vs. validation. Overfitting? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the model object\n",
    "\n",
    "# fit the model to our training data. We must specify the column in y_train, \n",
    "# since we have converted it to a dataframe from a series! \n",
    "\n",
    "\n",
    "# predict train\n",
    "\n",
    "# evaluate: rmse\n",
    "\n",
    "\n",
    "# predict validate\n",
    "\n",
    "# evaluate: rmse\n",
    "\n",
    "\n",
    "print(\"RMSE for GLM using Tweedie, power=1 & alpha=0\\nTraining/In-Sample: \", rmse_train, \n",
    "      \"\\nValidation/Out-of-Sample: \", rmse_validate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_df = metric_df.append(\n",
    "    {\n",
    "    'model': 'GLM Model power 1',\n",
    "    'rmse_validate': rmse_validate,\n",
    "    'r^2_validate': explained_variance_score(y_validate.G3, y_validate.G3_pred_glm)}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polynomial Regression\n",
    "\n",
    "Using sklearn.preprocessing.PolynommialFeatures() + sklearn.linear_model.LinearRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Create the new features, based on value indicated for degree for train, validate & test. \n",
    "\n",
    "2. Fit the Linear Regression model\n",
    "\n",
    "3. Predict using the transformed (squared or cubed, e.g.) features \n",
    "\n",
    "4. Evaluate using RMSE\n",
    "\n",
    "5. Repeat predictions and evaluation for validation.\n",
    "\n",
    "6. Compare RMSE train vs. validation. Overfitting?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **PolynomialFeatures**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the polynomial features to get a new set of features\n",
    "\n",
    "# fit and transform X_train_scaled\n",
    "\n",
    "# transform X_validate_scaled & X_test_scaled\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **LinearRegression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the model object\n",
    "# \n",
    "# make the thing\n",
    "# \n",
    "\n",
    "# fit the model to our training data. We must specify the column in y_train, \n",
    "# since we have converted it to a dataframe from a series! \n",
    "# \n",
    "# fit the thing\n",
    "# \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict train\n",
    "# \n",
    "# use the thing!\n",
    "# \n",
    "\n",
    "# evaluate: rmse\n",
    "# predict validate\n",
    "\n",
    "# evaluate: rmse\n",
    "\n",
    "print(\"RMSE for OLS using LinearRegression\\nTraining/In-Sample: \", rmse_train, \n",
    "      \"\\nValidation/Out-of-Sample: \", rmse_validate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate\n",
    "\n",
    "**Plotting Actual vs. Predicted Values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# y_validate.head()\n",
    "plt.figure(figsize=(16,8))\n",
    "plt.plot(y_validate.G3, y_validate.G3_pred_mean, alpha=.5, color=\"gray\", label='_nolegend_')\n",
    "plt.annotate(\"Baseline: Predict Using Mean\", (16, 9.5))\n",
    "plt.plot(y_validate.G3, y_validate.G3, alpha=.5, color=\"blue\", label='_nolegend_')\n",
    "plt.annotate(\"The Ideal Line: Predicted = Actual\", (.5, 3.5), rotation=15.5)\n",
    "\n",
    "plt.scatter(y_validate.G3, y_validate.G3_pred_lm, \n",
    "            alpha=.5, color=\"red\", s=100, label=\"Model: LinearRegression\")\n",
    "# plt.scatter(y_validate.G3, y_validate.G3_pred_glm, \n",
    "#             alpha=.5, color=\"yellow\", s=100, label=\"Model: TweedieRegressor\")\n",
    "plt.scatter(y_validate.G3, y_validate.G3_pred_lm2, \n",
    "            alpha=.5, color=\"green\", s=100, label=\"Model 2nd degree Polynomial\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Actual Final Grade\")\n",
    "plt.ylabel(\"Predicted Final Grade\")\n",
    "plt.title(\"Where are predictions more extreme? More modest?\")\n",
    "plt.annotate(\"The polynomial model appears to overreact to noise\", (2.0, -10))\n",
    "plt.annotate(\"The OLS model (LinearRegression)\\n appears to be most consistent\", (15.5, 3))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Residual Plots: Plotting the Errors in Predictions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# y_validate.head()\n",
    "plt.figure(figsize=(16,8))\n",
    "plt.axhline(label=\"No Error\")\n",
    "plt.scatter(y_validate.G3, y_validate.G3_pred_lm - y_validate.G3, \n",
    "            alpha=.5, color=\"red\", s=100, label=\"Model: LinearRegression\")\n",
    "plt.scatter(y_validate.G3, y_validate.G3_pred_glm - y_validate.G3, \n",
    "            alpha=.5, color=\"yellow\", s=100, label=\"Model: TweedieRegressor\")\n",
    "plt.scatter(y_validate.G3, y_validate.G3_pred_lm2 - y_validate.G3, \n",
    "            alpha=.5, color=\"green\", s=100, label=\"Model 2nd degree Polynomial\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Actual Final Grade\")\n",
    "plt.ylabel(\"Residual/Error: Predicted Grade - Actual Grade\")\n",
    "plt.title(\"Do the size of errors change as the actual value changes?\")\n",
    "plt.annotate(\"The polynomial model appears to overreact to noise\", (2.0, -10))\n",
    "plt.annotate(\"The OLS model (LinearRegression)\\n appears to be most consistent\", (15.5, 3))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Histograms**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# plot to visualize actual vs predicted. \n",
    "plt.figure(figsize=(16,8))\n",
    "plt.hist(y_validate.G3, color='blue', alpha=.5, label=\"Actual Final Grades\")\n",
    "plt.hist(y_validate.G3_pred_lm, color='red', alpha=.5, label=\"Model: LinearRegression\")\n",
    "# plt.hist(y_validate.G3_pred_glm, color='yellow', alpha=.5, label=\"Model: TweedieRegressor\")\n",
    "# plt.hist(y_validate.G3_pred_lm2, color='green', alpha=.5, label=\"Model 2nd degree Polynomial\")\n",
    "plt.xlabel(\"Final Grade (G3)\")\n",
    "plt.ylabel(\"Number of Students\")\n",
    "plt.title(\"Comparing the Distribution of Actual Grades to Distributions of Predicted Grades for the Top Models\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# addendum: Comparing models DF:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model Selection & Out-of-Sample Evaluation**\n",
    "\n",
    "Model selected: lm (using LinearRegression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict on test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate: rmse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
